Depth를 늘리는 이유 중 하나는 계산량을 일정하게 유지시키기
위해서 입니다(constant level of compute).
 
 inception module: "a good local network typology",  "network within a network" 라는 개념으로
local topology를 구현했고 이를 쌓아올렸습니다. => 그렇다면 이 방법의 문제가 무엇일까요? =>계산 비용
Inception module의 입력은 28 x 28 x 256 이었으나
출력은 28 x 28 x 672 이 된 것  
spatial dimention은 변하지 않았지만
depth가 엄청나게 불어난 것이죠
문제 해결 =>  "bottleneck layer" => 낮은 차원으로 보냄 -> 입력의 depth를
더 낮은 차원으로 projection 하는 것 => 입력의 depth를 줄이는 것 => 계산량은 1x1 conv를 통해 줄어듭니다.
보조분류기(auxiliary classifier) =>  보조 분류기를 이용해서 추가적인 그레디언트 신호를 흘려줍니다. => 
보조 분류기에서는 backprob 한 번만 함 => 각 출력에서의 gradient를 모두 계산한 다음 한번에 Backprop을 합니다.
Skip Connection은 가중치가 없으며 입력을
identity mapping으로 그대로 출력단으로 내보냅니다.
실제 레이어는 변화량(delta) 만 학습하면 됩니다.
입력 X에 대한 잔차(residual) 이라고 할 수 있죠
레이어의 출력과 Skip Connection의
출력이 같은 차원인지 => 두 경로의 출력 값 모두 같은 차원입니다. =>일반적으로는 같은 차원이 맞지만, 그렇지 않은 경우에는
Depth-wise padding으로 차원을 맞춰 줍니다.  
ResNet의 아이디어는 H(x) = F(x) + X 이니
F(x)를 학습시켜보면 어떨까? 하는 것입니다.F(x)가 Residual 입니다.
Global Average Pooling Layer 를 사용합니다.
GAP는 하나의 Map 전체를 Average Pooling 합니다.
ResNet의 경우 모델 Depth가
50 이상일 때 Bottleneck Layers를 도입합니다.
Network in Network 입니다.
각 Conv layer 안에 MLP(Multi-Layer Perceptron)
를 쌓습니다. FC-Layer 몇 개를 쌓는 것이죠



